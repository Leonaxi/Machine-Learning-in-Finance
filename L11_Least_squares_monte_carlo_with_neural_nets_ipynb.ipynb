{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonaxi/Machine-Learning-in-Finance/blob/main/L11_Least_squares_monte_carlo_with_neural_nets_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qW9RsdmLTiYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e26b294-2afd-4692-a20a-7f26669a7ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 33.0 MB/s \n",
            "\u001b[?25hCollecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.8-py3-none-any.whl (350 kB)\n",
            "\u001b[K     |████████████████████████████████| 350 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.1.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.22+cuda11.cudnn805)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.3.0)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.23)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.10)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.10.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.10.0)\n",
            "Installing collected packages: jmp, chex, optax, dm-haiku\n",
            "Successfully installed chex-0.1.5 dm-haiku-0.0.8 jmp-0.0.2 optax-0.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install optax dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import haiku as hk\n",
        "import optax\n",
        "\n",
        "optimizer = optax.adam\n",
        "lr = 1e-4\n",
        "\n",
        "\n",
        "Spot = jnp.array([38, 36, 35])   # stock price\n",
        "σ = jnp.array([0.2, .25, .3])     # stock volatility\n",
        "K = 40      # strike price\n",
        "r = 0.06    # risk free rate\n",
        "\n",
        "# change batch size(for training) and n(for evaluating)\n",
        "# n=100000 runs too slow, so we choose a small batch size for training\n",
        "# dont need to be that precise when train the model, that's the natual of stochastic descent\n",
        "# but if we have to evaluate it, we should have a precise number n=100000\n",
        "n = 100000  # Number of simualted paths   # evaluation batch size: larger\n",
        "batch_size = 512  # training batch_size: smaller\n",
        "\n",
        "\n",
        "m = 50      # number of exercise dates\n",
        "T = 1       # maturity\n",
        "order = 12   # Polynmial order\n",
        "Δt = T / m  # interval between two exercise dates\n",
        "\n",
        "n_stocks = 3\n",
        "\n",
        "# simulates one step of the stock price evolution\n",
        "def step(S, rng): # (sample, key): should specify key in future step function\n",
        "    # rng= jax.random.PRNGKey(0)  # do not use it, because it would not be stochastic(we want stochastic), will give the wrong numbers\n",
        "    ϵ = jax.random.normal(rng, S.shape) # key, shape: same dimension of S 生成符合正态分布的随机数以S的shape\n",
        "    dB = jnp.sqrt(Δt) * ϵ\n",
        "    S = S + r * S * Δt + σ * S * dB   # next instant will be this new S 遵循几何布朗运动的模拟\n",
        "    return S, S\n",
        "\n",
        "\n",
        "def payoff_put(S):  # option price\n",
        "    return jnp.maximum(K - jnp.max(S, 1), 0.)\n",
        "\n",
        "# normalize the input by subtract the mean and divide the sd or try a larger network --- more flexible network\n",
        "\n",
        "def model(Si):\n",
        "  out = (Si.reshape(-1,1) - 37.) / 5   # transform into metrix\n",
        "  out = hk.Linear(64)(out) # Si:hk would expecting it as a mitrix: #of col determine #of fetures. But the way we write the code is a vector\n",
        "  out = jax.nn.relu(out)\n",
        "\n",
        "  out = hk.Linear(64)(out)\n",
        "  out = jax.nn.relu(out)\n",
        "\n",
        "  out = hk.Linear(1)(out)\n",
        "  out = jnp.squeeze(out) # we want the result transfer from mitrix with one col to the vector\n",
        "  return out\n",
        "# squeeze: 从数组中删除单维度条目，即把shape=1的维度去掉，但对非但维度的维度不起作用\n"
      ],
      "metadata": {
        "id": "IdxVmv6DP8Fj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when doing gradient decent, always initilize your input, tend to accelerate the gredient descent"
      ],
      "metadata": {
        "id": "7wK_w2sUw3ym"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initilize the model\n",
        "\n",
        "# initilize with hk.transform model; the network is deterministic, no stochastic need to provide seed: withoud seed\n",
        "init, model = hk.without_apply_rng(hk.transform(model))\n",
        "rng = jax.random.PRNGKey(0)  # create a pseudo-random number key given a integer seed\n",
        "Θ = init(rng, jnp.ones([batch_size, n_stocks])) # n_stocks here =3\n",
        "\n",
        "# we have n_stock=49 nnet works, each element has 49 copies\n",
        "# we want to do every leaf for the python tree (1 nnet for 1 timeset for each regression)\n",
        "def stack(Θ):\n",
        "  return jnp.stack([Θ] * 49) \n",
        "\n",
        "# map a stack function over the theta tree, the output will be a pie tree, a data continer with same shape as theta\n",
        "# map a multi-input function over pytree args to produce a new pytree\n",
        "Θ = jax.tree_map(stack, Θ) \n",
        "\n",
        "# whenever you define a network parameter, you should initilize your optimizer state\n",
        "opt_state = optimizer(lr).init(Θ)\n"
      ],
      "metadata": {
        "id": "02TeM5PfICIu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0gC6lFfH1Ds"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @jax.jit\n",
        "# def update_gradient_descent(Θ, opt_state):\n",
        "\n",
        "#   def L(Θ):\n",
        "#     return compute_price(Θ)[1].sum()\n",
        "\n",
        "#   grad = jax.grad(L)(Θ)\n",
        "#   updates, opt_state = optimizer(lr).update(grad, opt_state)\n",
        "#   Θ = optax.apply_updates(Θ, updates)\n",
        "#   return Θ, opt_state\n"
      ],
      "metadata": {
        "id": "lIMXPHZgZbaA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWRGfWGUcxYy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSMC algorithm\n",
        "\n",
        "# compute_price: \n",
        "  # calculate the compute_price with large batchsize (n) when evaluating the model\n",
        "  # with small batchsize when training the model: (batch size =512)\n",
        "  # compute_price depends on desired batchsize\n",
        "# rng:\n",
        "  # if you always sample same data, will translate to lower performance model\n",
        "  # each iteration gredient descent, use dif random seeds\n",
        "def compute_price(Θ, batch_size, rng):  \n",
        "    S = jnp.column_stack([jnp.ones(batch_size) * Spot[i] for i in range(3)]) # Spot* jnp.ones\n",
        "    # rng = jax.random.PRNGKey(0)   # should random sampling dif data points each time: 每次512都不同\n",
        "    rng_vector = jax.random.split(rng, m)   # take the key and split the key into m new keys\n",
        "\n",
        "# jax.lax.scan():\n",
        "  # implement efficiency of dessert flow (function that you apply(step), initial state of stock price, input)\n",
        "  # initial: stock price change from step by step: initialized stock price and create a new stock price\n",
        "  # input: whatever you need to provide for the step function: here is random seed, for each time differnt key  \n",
        "    _, S = jax.lax.scan(step, S, rng_vector) \n",
        "    # 2 output: \n",
        "      # 1st: final stock price at exprision date (similate 50 times)\n",
        "      # 2nd: whatever the term return the second output (自定义，见step,里面说的是S)\n",
        "\n",
        "    discount = jnp.exp(-r * Δt)\n",
        "\n",
        "    # Very last date\n",
        "    value_if_exercise = payoff_put(S[-1])\n",
        "    discounted_future_cashflows = value_if_exercise * discount\n",
        "\n",
        "# state: the thing change from iteration to iteration\n",
        "# input: things have to provide for this loop, depends on the index, i of the iteration\n",
        "    def core(state, input):\n",
        "        discounted_future_cashflows = state \n",
        "        Si, Θi = input\n",
        "        Y = discounted_future_cashflows  \n",
        "\n",
        "        #def model(Θi, Si):\n",
        "        #  X= chebyshev_basis(scale(Si), order)\n",
        "        #  return X @ Θi\n",
        "        value_if_wait = model(Θi, Si) # write it as a function\n",
        "\n",
        "        mse = jnp.mean((value_if_wait - discounted_future_cashflows)**2)\n",
        "        value_if_exercise = payoff_put(Si)\n",
        "        exercise = value_if_exercise >= value_if_wait  # make decision based on comparision\n",
        "        discounted_future_cashflows = discount * jnp.where(\n",
        "            exercise,\n",
        "            value_if_exercise,\n",
        "            discounted_future_cashflows)\n",
        "\n",
        "        return discounted_future_cashflows, mse\n",
        " \n",
        "\n",
        "    # Proceed recursively\n",
        "    S = jnp.flip(S, 0)[1:] # delete the first row of metrix\n",
        "    inputs = S, Θ\n",
        "    discounted_future_cashflows, mse = jax.lax.scan(core, discounted_future_cashflows, inputs)\n",
        "\n",
        "    return discounted_future_cashflows.mean(), mse\n",
        "\n",
        "#print(compute_price(Θ, batch_size, rng))\n",
        "\n",
        "\n",
        "\n",
        "    # 1st array: mean of discounted future casflows\n",
        "    ## 3.95 should gradually increse from the initial value 3.95 to 4.47\n",
        "    # if you use the wrong Θ (continuous value), you will have wrong comparision on if should exercise\n",
        "    # follow the strategy with wrong continuous value, should produce a suboptimal value for the future cash flows\n",
        "    ## Θ minimize mse： \n",
        "    # 2nd array: ([mse of 1st regression, mse of 2nd regression])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "90GTflZcYF4-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit compute_price()"
      ],
      "metadata": {
        "id": "jNgPX60ZMY6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2XHMYpaMY7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function: training data, use small batch size = 500\n",
        "def L(Θ, rng):\n",
        "  mse = compute_price(Θ, batch_size, rng)[1]  # compute_price 里面第二个变量是[1]mse\n",
        "  return mse.sum()\n",
        "\n",
        "# apply gredient descent with opt_state\n",
        "@jax.jit  # want to run fast\n",
        "def gradient_descent_step(Θ, opt_state, rng):   # update gredient descent by using optimize_state\n",
        "  rng, _ = jax.random.split(rng) # split differnet new seed and return the (rng) new seed, next time will use new random seed to run a code (no overfitting)\n",
        "  grad = jax.grad(L)(Θ, rng)    # compute the gradient using jax.grad(loss function) to evaluate Θ\n",
        "  updates, opt_state = optimizer(lr).update(grad, opt_state)  # compute updates, opt_state = optimizer(learning rate) + apply gradient and previous optimized state\n",
        "  Θ = optax.apply_updates(Θ, updates)\n",
        "  return Θ, opt_state, rng  # return new parameter(metrix), new optimal state\n",
        "\n",
        "# gradient_descent_step(Θ, opt_state, rng)  # run to check if it works\n",
        "\n"
      ],
      "metadata": {
        "id": "pB3FhsM4an5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oI9xOgMMeAHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define evaluation function: use large batch size = n \n",
        "  # if you use right /theta means you have right decision on exrcising\n",
        "  # 所以 we want to keep track discounted_future_cashflows.mean()--- can calculate the option price\n",
        "  # 所以 we want to check how well dcf \"[0]\" is, but not mse \"[1]\" because it is not train and test result\n",
        "@jax.jit\n",
        "def evaluation(Θ):\n",
        "  rng = jax.random.PRNGKey(0)  # random seed\n",
        "  return compute_price(Θ, n, rng)[0] # return discounted_future_cashflows.mean()\n",
        "\n",
        "\n",
        "# iteration\n",
        "rng = jax.random.PRNGKey(0)\n",
        "for iteration in range(10000):\n",
        "  Θ, opt_state, rng = gradient_descent_step(Θ, opt_state, rng)\n",
        "\n",
        "# to monitor process to check how well my code is performing, for every 1000 iteration, run the code to see how well it does\n",
        "  if iteration % 100 == 0:\n",
        "    metric = evaluation(Θ)  # θ defines how I compute the contiuns values, depends how I compute the prices\n",
        "    print(metric)\n",
        "  \n",
        "  # numbers are going up from 3.+ to 4.47, it is what we expected"
      ],
      "metadata": {
        "id": "PDmoKQ5y_tkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3u2wvaT-_Mtw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}